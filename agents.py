# agents.py
import llm_api
from prompts import Prompts
import utils

class ProblemAnalyst:
    """é—®é¢˜åˆ†æžæ™ºèƒ½ä½“ (è®ºæ–‡ 3.2)"""
    def analyze(self, problem: str) -> str:
        print(f"  [Analyst] Analyzing problem...")
        prompt = Prompts.ANALYSIS_PROMPT.format(problem=problem)
        response = llm_api.call_llm(prompt)
        # è¿™é‡Œæˆ‘ä»¬ç›´æŽ¥è¿”å›žæ•´ä¸ªåˆ†æžç»“æžœï¼Œå› ä¸ºå®ƒå°†å®Œæ•´åœ°ä½œä¸ºPlannerçš„è¾“å…¥
        return response

class TaskPlanner:
    """ä»»åŠ¡è§„åˆ’æ™ºèƒ½ä½“ (è®ºæ–‡ 3.3)"""
    def initial_plan(self, problem: str, analysis_result: str) -> str:
        print(f"  [Planner] Creating initial plan...")
        prompt = Prompts.PLANNING_PROMPT.format(
            problem=problem,
            analysis_result=analysis_result
        )
        response = llm_api.call_llm(prompt)
        return response

    def reflection_plan(self, problem: str, prev_plan: str, prev_code: str, test_report: str) -> str:
        """å¯¹åº”è®ºæ–‡ä¸­çš„åæ€æœºåˆ¶ (Reflection Loop)"""
        print(f"  [Planner] Reflecting on failure...")
        prompt = Prompts.REFLECTION_PROMPT.format(
            problem=problem,
            previous_plan=prev_plan,
            previous_code=prev_code,
            test_report=test_report
        )
        response = llm_api.call_llm(prompt)
        # åªéœ€è¦æå–ä¿®æ­£åŽçš„è®¡åˆ’éƒ¨åˆ†
        modified_plan = utils.extract_section(response, "## Modified Planning:")
        # å¦‚æžœæå–å¤±è´¥ï¼ˆæ¨¡åž‹æ²¡éµå¾ªæ ¼å¼ï¼‰ï¼Œåˆ™å…œåº•è¿”å›žæ•´ä¸ªå›žå¤
        return modified_plan if modified_plan else response

class CodeGenerator:
    """ä»£ç ç”Ÿæˆæ™ºèƒ½ä½“ (è®ºæ–‡ 3.4)"""
    def generate_code(self, problem: str, plan: str) -> str:
        print(f"  [Coder] Generating code...")
        prompt = Prompts.CODING_PROMPT.format(
            problem=problem,
            plan=plan
        )
        response = llm_api.call_llm(prompt)
        code = utils.extract_code(response)


        # ã€æ–°å¢žã€‘æ•…æ„æžç ´åï¼šå¦‚æžœæ˜¯åœ¨æµ‹è¯•æ–æ³¢é‚£å¥‘ï¼Œå¼ºåˆ¶åœ¨ä»£ç æœ«å°¾åŠ ä¸ª Bug
        if "fibonacci" in problem.lower():
            print("  [ðŸ˜ˆ Sabotage] Injecting a bug to test Debugger...")
            code = code.replace("return", "retrun") # æ•…æ„ç®—é”™ç»“æžœ

        return code

class CodeDebugger:
    """ä»£ç è°ƒè¯•æ™ºèƒ½ä½“ (è®ºæ–‡ 3.5)"""
    def debug(self, problem: str, plan: str, code: str, test_report: str) -> str:
        print(f"  [Debugger] Fixing code...")
        prompt = Prompts.DEBUGGING_PROMPT.format(
            problem=problem,
            plan=plan,
            code=code,
            test_report=test_report
        )
        response = llm_api.call_llm(prompt)
        fixed_code = utils.extract_code(response)
        return fixed_code